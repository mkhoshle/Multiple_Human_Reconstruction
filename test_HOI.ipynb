{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757e6e8f-70b1-4a93-a668-89f99d9292bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaml_timestamp  /z/home/mkhoshle/Human_object_transform/active_configs/active_context_2023-06-11_16_30_05.yaml\n",
      "No configs_yml is set, set it to the default --configs_yml=configs/image.yml\n",
      "yaml_timestamp  /z/home/mkhoshle/Human_object_transform/active_configs/active_context_2023-06-11_16_30_05.yaml\n",
      "yaml_timestamp  /z/home/mkhoshle/Human_object_transform/active_configs/active_context_2023-06-11_16_30_05.yaml\n",
      "/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/config.py\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import logging\n",
    "from HumanObj_videos_ResNet.lib.config import ConfigContext, parse_args, args\n",
    "from HumanObj_videos_ResNet.predict.image import Image_processor\n",
    "\n",
    "from HumanObj_videos_ResNet.lib.models import build_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "import torch, torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc61c428-55d5-4458-8137-084e75c6a92d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ConfigContext.parsed_args = parse_args([\"--configs_yml=configs/image.yml\",\n",
    "                                        '--inputs=demo/images', \n",
    "                                        '--output_dir=demo/image_results', \n",
    "                                        '--renderer=pytorch3d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a120025-a9c7-4593-af88-80b3e905ba6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = \"trained_models/resnet_cm32_V1_epoch_9.pkl\"  \n",
    "# if os.path.exists(path):\n",
    "#     pretrained_model = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c25f26-94d9-46c8-af3f-c622541fc19d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pretrained_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6451b39c-dad3-4342-8c21-b35f503231cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pretrained_model['position_embedding.row_embed.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38031f4-b84f-4c09-a405-58d481dd1885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'tab': 'resnet_cm32_process_images', 'configs_yml': 'configs/image.yml', 'inputs': 'demo/images', 'output_dir': 'demo/image_results', 'interactive_vis': False, 'show_largest_person_only': False, 'show_mesh_stand_on_image': False, 'soi_camera': 'far', 'make_tracking': False, 'temporal_optimization': False, 'save_dict_results': True, 'save_visualization_on_img': True, 'fps_save': 24, 'character': 'smpl', 'renderer': 'pytorch3d', 'f': None, 'model_return_loss': False, 'model_version': 1, 'multi_person': True, 'new_training': False, 'perspective_proj': False, 'FOV': 60, 'focal_length': 443.4, 'lr_backbone': 0.0003, 'lr': 0.0003, 'adjust_lr_factor': 0.1, 'weight_decay': 1e-06, 'epoch': 50, 'fine_tune': True, 'GPUS': 0, 'batch_size': 64, 'input_size': 512, 'master_batch_size': -1, 'nw': 4, 'optimizer_type': 'Adam', 'pretrain': 'imagenet', 'fix_backbone_training_scratch': False, 'dropout': 0.1, 'pre_norm': True, 'backbone_num_channels': 64, 'track_attention': False, 'enc_layers': 3, 'dec_layers': 3, 'deformable': False, 'dim_feedforward': 256, 'nheads': 8, 'num_feature_levels': 1, 'masks': False, 'dilation': False, 'hidden_dim': 64, 'hidden_dim_pos': 64, 'position_embedding': 'learned', 'backbone': 'resnet', 'model_precision': 'fp32', 'deconv_num': 0, 'head_block_num': 2, 'merge_smpl_camera_head': False, 'use_coordmaps': True, 'hrnet_pretrain': '/z/home/mkhoshle/Human_object_transform/trained_models/pretrain_hrnet.pkl', 'resnet_pretrain': '/z/home/mkhoshle/Human_object_transform/trained_models/pretrain_resnet.pkl', 'loss_thresh': 1000, 'max_supervise_num': -1, 'supervise_cam_params': False, 'match_preds_to_gts_for_supervision': True, 'matching_mode': 'all', 'supervise_global_rot': False, 'HMloss_type': 'MSE', 'eval': True, 'eval_datasets': 'pw3d', 'val_batch_size': 4, 'test_interval': 10, 'fast_eval_iter': -1, 'top_n_error_vis': 6, 'eval_2dpose': False, 'calc_pck': False, 'PCK_thresh': 150, 'calc_PVE_error': False, 'centermap_size': 32, 'centermap_conf_thresh': 0.2, 'collision_aware_centermap': True, 'collision_factor': 0.2, 'center_def_kp': True, 'local_rank': 0, 'distributed_training': False, 'distillation_learning': False, 'teacher_model_path': '/export/home/suny/CenterMesh/trained_models/3dpw_88_57.8.pkl', 'print_freq': 50, 'model_path': 'trained_models/resnet_cm32_V1_epoch_68.pkl', 'log_path': '/z/home/mkhoshle/log/', 'learn_2dpose': False, 'learn_AE': False, 'learn_kp2doffset': False, 'shuffle_crop_mode': False, 'shuffle_crop_ratio_3d': 0.9, 'shuffle_crop_ratio_2d': 0.1, 'Synthetic_occlusion_ratio': 0, 'color_jittering_ratio': 0.2, 'rotate_prob': 0.2, 'dataset_rootdir': '/z/home/mkhoshle/dataset/ROMP_datasets', 'dataset': 'h36m,mpii,coco,aich,up,ochuman,lsp,movi', 'voc_dir': '/z/home/mkhoshle/dataset/VOCdevkit/VOC2012/', 'max_person': 64, 'homogenize_pose_space': False, 'use_eft': True, 'smpl_mesh_root_align': True, 'Rot_type': '6D', 'rot_dim': 6, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl', 'smpl_J_reg_h37m_path': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl/J_regressor_extra.npy', 'smpl_uvmap': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl/smpl_vt_ft.npz', 'wardrobe': '/z/home/mkhoshle/Human_object_transform/model_data/wardrobe', 'mesh_cloth': 'ghostwhite', 'nvxia_model_path': '/z/home/mkhoshle/Human_object_transform/model_data/characters/nvxia', 'track_memory_usage': False, 'adjust_lr_epoch': [], 'kernel_sizes': [5], 'nw_eval': 4, 'collect_subdirs': False, 'save_mesh': False, 'save_centermap': True}\n",
      "INFO:root:------------------------------------------------------------------\n",
      "INFO:root:Loading pytorch3d renderer as visualizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualize in gpu mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:start building model.\n",
      "INFO:root:missing parameters of layers:[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading\n",
      "Using HOBJ v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using fine_tune model: trained_models/resnet_cm32_V1_epoch_68.pkl\n",
      "INFO:root:copy param _result_parser.params_map_parser.smpl_model.betas failed, mismatched\n",
      "INFO:root:missing parameters of layers:[]\n",
      "WARNING:root:Successfully loaded the model trained_models/resnet_cm32_V1_epoch_68.pkl!\n",
      "INFO:root:Train all layers, except: ['_result_parser.params_map_parser.smpl_model.betas']\n",
      "INFO:root:gathering datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 0.2\n",
      "Initialization finished!\n",
      "Processing demo/images, saving to demo/image_results\n",
      "Loading 3 images to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:gathering datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 0 images to process\n",
      "Loading 0 images to process\n",
      "Loading 0 images to process\n",
      "Loading 3 images to process\n",
      "torch.Size([3, 512, 512, 3]) torch.Size([3, 512, 512, 3])\n",
      "cfg {'mode': 'parsing', 'calc_loss': False}\n",
      "['demo/images/image_00090.jpg', 'demo/images/image_00298.jpg', 'demo/images/image_00401.jpg']\n",
      "tensor([1., 1., 1.], device='cuda:0') klkll\n",
      "dict_keys(['params_maps', 'center_map', 'detection_flag', 'params_pred', 'centers_pred', 'centers_conf', 'reorganize_idx', 'params', 'verts', 'j3d', 'joints_smpl24', 'joints_h36m17', 'verts_camed', 'pj2d', 'cam_trans', 'pj2d_org', 'meta_data']) 666\n",
      "outputs dict_keys(['params_maps', 'center_map', 'detection_flag', 'params_pred', 'centers_pred', 'centers_conf', 'reorganize_idx', 'params', 'verts', 'j3d', 'joints_smpl24', 'joints_h36m17', 'verts_camed', 'pj2d', 'cam_trans', 'pj2d_org', 'meta_data'])\n",
      "center_confs [[0.9272]\n",
      " [0.751 ]\n",
      " [0.8735]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/z/home/mkhoshle/env/HOI/lib/python3.9/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['params_maps', 'center_map', 'detection_flag', 'params_pred', 'centers_pred', 'centers_conf', 'reorganize_idx', 'params', 'verts', 'j3d', 'joints_smpl24', 'joints_h36m17', 'verts_camed', 'pj2d', 'cam_trans', 'pj2d_org', 'meta_data'])\n",
      "tensor([[16, 11],\n",
      "        [18, 12],\n",
      "        [17, 12]], device='cuda:0')\n",
      "True\n",
      "odict_keys(['org_img', 'mesh_rendering_orgimgs', 'mesh_rendering_imgs', 'centermap'])\n",
      "Processed 0 / 3 images\n"
     ]
    }
   ],
   "source": [
    "# second, run the code\n",
    "processor = Image_processor(args_set=args())\n",
    "inputs = args().inputs\n",
    "results = processor.run(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61cba5a-ab90-4532-88b1-9c1166c5dca0",
   "metadata": {},
   "source": [
    "# from IPython.display import Image, display\n",
    "import glob\n",
    "for img_path in glob.glob('demo/image_results2/*.jpg'):\n",
    "    display(Image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b3903-a6ac-47e3-8c37-963c7cf31c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d7e2d-6a62-4e3a-a527-ba7d89db1637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566db61-881a-4d47-a9bb-c870ab25c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, if you want to export the model to ONNX:\n",
    "# torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa920d7-5e98-4ac2-a74a-9b8828a3847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as py_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90a561-58ef-4864-94d8-11b5b45b9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HumanObj_videos_ResNet.lib.models.BackboneWithFPN import _resnet_fpn_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a7099-5ae0-48eb-aca0-eb15ef3964b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = py_models.resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bcf50-8c53-4da8-b47d-a67fda3d9b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = _resnet_fpn_extractor(backbone, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f574e82-a82f-4e94-b9d6-5300c6173b23",
   "metadata": {},
   "source": [
    "## Plot Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15c62e-579b-40e2-849e-98e5f0f9ab9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"val.txt\"\n",
    "df_val = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654a5d3-acf0-4918-a541-1d696bcd18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x.rstrip(';')\n",
    "df_val[[1]] = df_val[[1]].applymap(f)\n",
    "    \n",
    "f = lambda x: x.replace(\"MPJPE:\", \"\")\n",
    "df_val[[1]] = df_val[[1]].applymap(f)\n",
    "\n",
    "df_val[[\"MPJPE_mean\",\"MPJPE_var\"]] = df_val[1].str.split(\"|\", expand = True)\n",
    "\n",
    "f = lambda x: float(x)\n",
    "df_val[[\"MPJPE_mean\",\"MPJPE_var\"]] = df_val[[\"MPJPE_mean\",\"MPJPE_var\"]].applymap(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebb563-0113-4ad5-83bd-4754a89459c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x.rstrip(';')\n",
    "df_val[[3]] = df_val[[3]].applymap(f)\n",
    "    \n",
    "f = lambda x: x.replace(\"PAMPJPE:\", \"\")\n",
    "df_val[[3]] = df_val[[3]].applymap(f)\n",
    "\n",
    "df_val[[\"PAMPJPE_mean\",\"PAMPJPE_var\"]] = df_val[3].str.split(\"|\", expand = True)\n",
    "\n",
    "f = lambda x: float(x)\n",
    "df_val[[\"PAMPJPE_mean\",\"PAMPJPE_var\"]] = df_val[[\"PAMPJPE_mean\",\"PAMPJPE_var\"]].applymap(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75072630-161a-47db-b2b6-1b70d080b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val.drop([0,1,2,3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249f133-c32d-451f-83d7-7cec240f1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['step'] = [i*2000 for i in range(len(df_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf91ef1-1be6-43ee-820e-14fcfcfcc859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48d8c9-1934-4f51-a0cf-1c8d80331f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.size'] = 9\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "\n",
    "yerrs = [np.sqrt(df_val['MPJPE_var'].values.transpose().tolist()),np.sqrt(df_val['PAMPJPE_var'].values.transpose().tolist())]\n",
    "\n",
    "df_val.plot(x='step', y=['PAMPJPE_mean','MPJPE_mean'], color=['blue','red'], yerr=yerrs, ax=axes) \n",
    "\n",
    "# df_val.loc[:,['PAMPJPE_mean','MPJPE_mean']].plot(color=['blue','red'],yerr=yerrs,ax=axes) \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('ticks')\n",
    "axes.set_xlabel('$Iterations$')   \n",
    "axes.set_ylabel('Performance')\n",
    "\n",
    "axes.set_ylim(0, 400)\n",
    "# axes.set_xlim(0, 220)\n",
    "axes.yaxis.grid(which='major', linewidth='0.5', color='grey')\n",
    "axes.xaxis.grid(which='major', linewidth='0.5', color='grey')\n",
    "axes.yaxis.grid(which='minor', linewidth='0.5', color='grey')\n",
    "axes.xaxis.grid(which='minor', linewidth='0.5', color='grey') \n",
    "sns.despine(offset=10, ax=axes)\n",
    "\n",
    "# lgd = plt.legend(['IO','Compute','Opening Trajectory','Communication'],loc='upper center', bbox_to_anchor=(0.43, -0.38),\n",
    "                 # frameon=False, shadow=False, ncol=2, prop={'size':12})\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.4)\n",
    "\n",
    "fig.savefig('{}.pdf'.format('validation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447165b-a7bf-4a6e-9668-6ecbe8c69e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8935a41-5b1c-4d40-858b-7dd73c0fc406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
