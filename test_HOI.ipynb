{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757e6e8f-70b1-4a93-a668-89f99d9292bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import logging\n",
    "from HumanObj_videos_ResNet.lib.config import ConfigContext, parse_args, args\n",
    "from HumanObj_videos_ResNet.predict.image import Image_processor\n",
    "\n",
    "from HumanObj_videos_ResNet.lib.models import build_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc61c428-55d5-4458-8137-084e75c6a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigContext.parsed_args = parse_args([\"--configs_yml=configs/image.yml\",\n",
    "                                        '--inputs=demo/images', \n",
    "                                        '--output_dir=demo/image_results2', \n",
    "                                        '--renderer=pytorch3d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38031f4-b84f-4c09-a405-58d481dd1885",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'tab': 'resnet_cm16_process_images', 'configs_yml': 'configs/image.yml', 'inputs': 'demo/images', 'output_dir': 'demo/image_results2', 'interactive_vis': True, 'show_largest_person_only': False, 'show_mesh_stand_on_image': True, 'soi_camera': 'far', 'make_tracking': False, 'temporal_optimization': False, 'save_dict_results': True, 'save_visualization_on_img': True, 'fps_save': 24, 'character': 'smpl', 'renderer': 'pytorch3d', 'f': None, 'model_return_loss': False, 'model_version': 1, 'multi_person': True, 'new_training': False, 'perspective_proj': False, 'FOV': 60, 'focal_length': 443.4, 'lr_backbone': 0.0003, 'lr': 0.0003, 'adjust_lr_factor': 0.1, 'weight_decay': 1e-06, 'epoch': 50, 'fine_tune': True, 'GPUS': 0, 'batch_size': 64, 'input_size': 512, 'master_batch_size': -1, 'nw': 4, 'optimizer_type': 'Adam', 'pretrain': 'simplebaseline', 'fix_backbone_training_scratch': False, 'dropout': 0.1, 'pre_norm': False, 'backbone_num_channels': 64, 'track_attention': False, 'enc_layers': 6, 'dec_layers': 6, 'dim_feedforward': 2048, 'nheads': 8, 'num_feature_levels': 1, 'masks': False, 'dilation': False, 'hidden_dim': 256, 'hidden_dim_pos': 256, 'position_embedding': 'learned', 'backbone': 'resnet', 'model_precision': 'fp16', 'deconv_num': 0, 'head_block_num': 2, 'merge_smpl_camera_head': False, 'use_coordmaps': True, 'hrnet_pretrain': '/z/home/mkhoshle/Human_object_transform/trained_models/pretrain_hrnet.pkl', 'resnet_pretrain': '/z/home/mkhoshle/Human_object_transform/trained_models/pretrain_resnet.pkl', 'loss_thresh': 1000, 'max_supervise_num': -1, 'supervise_cam_params': False, 'match_preds_to_gts_for_supervision': True, 'matching_mode': 'all', 'supervise_global_rot': False, 'HMloss_type': 'MSE', 'eval': False, 'eval_datasets': 'pw3d', 'val_batch_size': 4, 'test_interval': 2000, 'fast_eval_iter': -1, 'top_n_error_vis': 6, 'eval_2dpose': False, 'calc_pck': False, 'PCK_thresh': 150, 'calc_PVE_error': False, 'centermap_size': 16, 'centermap_conf_thresh': 0.25, 'collision_aware_centermap': False, 'collision_factor': 0.2, 'center_def_kp': True, 'local_rank': 0, 'distributed_training': False, 'distillation_learning': False, 'teacher_model_path': '/export/home/suny/CenterMesh/trained_models/3dpw_88_57.8.pkl', 'print_freq': 50, 'model_path': 'trained_models/resnet_cm16_V1_epoch_10.pkl', 'log_path': '/z/home/mkhoshle/log/', 'learn_2dpose': False, 'learn_AE': False, 'learn_kp2doffset': False, 'shuffle_crop_mode': False, 'shuffle_crop_ratio_3d': 0.9, 'shuffle_crop_ratio_2d': 0.1, 'Synthetic_occlusion_ratio': 0, 'color_jittering_ratio': 0.2, 'rotate_prob': 0.2, 'dataset_rootdir': '/z/home/mkhoshle/dataset/ROMP_datasets', 'dataset': 'h36m,mpii,coco,aich,up,ochuman,lsp,movi', 'voc_dir': '/z/home/mkhoshle/dataset/VOCdevkit/VOC2012/', 'max_person': 64, 'homogenize_pose_space': False, 'use_eft': True, 'smpl_mesh_root_align': True, 'Rot_type': '6D', 'rot_dim': 6, 'cam_dim': 3, 'beta_dim': 10, 'smpl_joint_num': 22, 'smpl_model_path': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl', 'smpl_J_reg_h37m_path': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl/J_regressor_h36m.npy', 'smpl_J_reg_extra_path': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl/J_regressor_extra.npy', 'smpl_uvmap': '/z/home/mkhoshle/Human_object_transform/model_data/parameters/smpl/smpl_vt_ft.npz', 'wardrobe': '/z/home/mkhoshle/Human_object_transform/model_data/wardrobe', 'mesh_cloth': 'ghostwhite', 'nvxia_model_path': '/z/home/mkhoshle/Human_object_transform/model_data/characters/nvxia', 'track_memory_usage': False, 'adjust_lr_epoch': [], 'kernel_sizes': [5], 'nw_eval': 2, 'collect_subdirs': False, 'save_mesh': True, 'save_centermap': True}\n",
      "INFO:root:------------------------------------------------------------------\n",
      "INFO:root:Loading pytorch3d renderer as visualizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualize in gpu mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:start building model.\n",
      "INFO:root:missing parameters of layers:[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/z/home/mkhoshle/Human_object_transform/trained_models/pretrain_resnet.pkl\n",
      "Using HOBJ v1\n",
      "Confidence: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using fine_tune model: trained_models/resnet_cm16_V1_epoch_10.pkl\n",
      "INFO:root:missing parameters of layers:['backbone.0.conv1.weight', 'backbone.0.bn1.weight', 'backbone.0.bn1.bias', 'backbone.0.bn1.running_mean', 'backbone.0.bn1.running_var', 'backbone.0.bn1.num_batches_tracked', 'backbone.0.layer1.0.conv1.weight', 'backbone.0.layer1.0.bn1.weight', 'backbone.0.layer1.0.bn1.bias', 'backbone.0.layer1.0.bn1.running_mean', 'backbone.0.layer1.0.bn1.running_var', 'backbone.0.layer1.0.bn1.num_batches_tracked', 'backbone.0.layer1.0.conv2.weight', 'backbone.0.layer1.0.bn2.weight', 'backbone.0.layer1.0.bn2.bias', 'backbone.0.layer1.0.bn2.running_mean', 'backbone.0.layer1.0.bn2.running_var', 'backbone.0.layer1.0.bn2.num_batches_tracked', 'backbone.0.layer1.0.conv3.weight', 'backbone.0.layer1.0.bn3.weight', 'backbone.0.layer1.0.bn3.bias', 'backbone.0.layer1.0.bn3.running_mean', 'backbone.0.layer1.0.bn3.running_var', 'backbone.0.layer1.0.bn3.num_batches_tracked', 'backbone.0.layer1.0.downsample.0.weight', 'backbone.0.layer1.0.downsample.1.weight', 'backbone.0.layer1.0.downsample.1.bias', 'backbone.0.layer1.0.downsample.1.running_mean', 'backbone.0.layer1.0.downsample.1.running_var', 'backbone.0.layer1.0.downsample.1.num_batches_tracked', 'backbone.0.layer1.1.conv1.weight', 'backbone.0.layer1.1.bn1.weight', 'backbone.0.layer1.1.bn1.bias', 'backbone.0.layer1.1.bn1.running_mean', 'backbone.0.layer1.1.bn1.running_var', 'backbone.0.layer1.1.bn1.num_batches_tracked', 'backbone.0.layer1.1.conv2.weight', 'backbone.0.layer1.1.bn2.weight', 'backbone.0.layer1.1.bn2.bias', 'backbone.0.layer1.1.bn2.running_mean', 'backbone.0.layer1.1.bn2.running_var', 'backbone.0.layer1.1.bn2.num_batches_tracked', 'backbone.0.layer1.1.conv3.weight', 'backbone.0.layer1.1.bn3.weight', 'backbone.0.layer1.1.bn3.bias', 'backbone.0.layer1.1.bn3.running_mean', 'backbone.0.layer1.1.bn3.running_var', 'backbone.0.layer1.1.bn3.num_batches_tracked', 'backbone.0.layer1.2.conv1.weight', 'backbone.0.layer1.2.bn1.weight', 'backbone.0.layer1.2.bn1.bias', 'backbone.0.layer1.2.bn1.running_mean', 'backbone.0.layer1.2.bn1.running_var', 'backbone.0.layer1.2.bn1.num_batches_tracked', 'backbone.0.layer1.2.conv2.weight', 'backbone.0.layer1.2.bn2.weight', 'backbone.0.layer1.2.bn2.bias', 'backbone.0.layer1.2.bn2.running_mean', 'backbone.0.layer1.2.bn2.running_var', 'backbone.0.layer1.2.bn2.num_batches_tracked', 'backbone.0.layer1.2.conv3.weight', 'backbone.0.layer1.2.bn3.weight', 'backbone.0.layer1.2.bn3.bias', 'backbone.0.layer1.2.bn3.running_mean', 'backbone.0.layer1.2.bn3.running_var', 'backbone.0.layer1.2.bn3.num_batches_tracked', 'backbone.0.layer2.0.conv1.weight', 'backbone.0.layer2.0.bn1.weight', 'backbone.0.layer2.0.bn1.bias', 'backbone.0.layer2.0.bn1.running_mean', 'backbone.0.layer2.0.bn1.running_var', 'backbone.0.layer2.0.bn1.num_batches_tracked', 'backbone.0.layer2.0.conv2.weight', 'backbone.0.layer2.0.bn2.weight', 'backbone.0.layer2.0.bn2.bias', 'backbone.0.layer2.0.bn2.running_mean', 'backbone.0.layer2.0.bn2.running_var', 'backbone.0.layer2.0.bn2.num_batches_tracked', 'backbone.0.layer2.0.conv3.weight', 'backbone.0.layer2.0.bn3.weight', 'backbone.0.layer2.0.bn3.bias', 'backbone.0.layer2.0.bn3.running_mean', 'backbone.0.layer2.0.bn3.running_var', 'backbone.0.layer2.0.bn3.num_batches_tracked', 'backbone.0.layer2.0.downsample.0.weight', 'backbone.0.layer2.0.downsample.1.weight', 'backbone.0.layer2.0.downsample.1.bias', 'backbone.0.layer2.0.downsample.1.running_mean', 'backbone.0.layer2.0.downsample.1.running_var', 'backbone.0.layer2.0.downsample.1.num_batches_tracked', 'backbone.0.layer2.1.conv1.weight', 'backbone.0.layer2.1.bn1.weight', 'backbone.0.layer2.1.bn1.bias', 'backbone.0.layer2.1.bn1.running_mean', 'backbone.0.layer2.1.bn1.running_var', 'backbone.0.layer2.1.bn1.num_batches_tracked', 'backbone.0.layer2.1.conv2.weight', 'backbone.0.layer2.1.bn2.weight', 'backbone.0.layer2.1.bn2.bias', 'backbone.0.layer2.1.bn2.running_mean', 'backbone.0.layer2.1.bn2.running_var', 'backbone.0.layer2.1.bn2.num_batches_tracked', 'backbone.0.layer2.1.conv3.weight', 'backbone.0.layer2.1.bn3.weight', 'backbone.0.layer2.1.bn3.bias', 'backbone.0.layer2.1.bn3.running_mean', 'backbone.0.layer2.1.bn3.running_var', 'backbone.0.layer2.1.bn3.num_batches_tracked', 'backbone.0.layer2.2.conv1.weight', 'backbone.0.layer2.2.bn1.weight', 'backbone.0.layer2.2.bn1.bias', 'backbone.0.layer2.2.bn1.running_mean', 'backbone.0.layer2.2.bn1.running_var', 'backbone.0.layer2.2.bn1.num_batches_tracked', 'backbone.0.layer2.2.conv2.weight', 'backbone.0.layer2.2.bn2.weight', 'backbone.0.layer2.2.bn2.bias', 'backbone.0.layer2.2.bn2.running_mean', 'backbone.0.layer2.2.bn2.running_var', 'backbone.0.layer2.2.bn2.num_batches_tracked', 'backbone.0.layer2.2.conv3.weight', 'backbone.0.layer2.2.bn3.weight', 'backbone.0.layer2.2.bn3.bias', 'backbone.0.layer2.2.bn3.running_mean', 'backbone.0.layer2.2.bn3.running_var', 'backbone.0.layer2.2.bn3.num_batches_tracked', 'backbone.0.layer2.3.conv1.weight', 'backbone.0.layer2.3.bn1.weight', 'backbone.0.layer2.3.bn1.bias', 'backbone.0.layer2.3.bn1.running_mean', 'backbone.0.layer2.3.bn1.running_var', 'backbone.0.layer2.3.bn1.num_batches_tracked', 'backbone.0.layer2.3.conv2.weight', 'backbone.0.layer2.3.bn2.weight', 'backbone.0.layer2.3.bn2.bias', 'backbone.0.layer2.3.bn2.running_mean', 'backbone.0.layer2.3.bn2.running_var', 'backbone.0.layer2.3.bn2.num_batches_tracked', 'backbone.0.layer2.3.conv3.weight', 'backbone.0.layer2.3.bn3.weight', 'backbone.0.layer2.3.bn3.bias', 'backbone.0.layer2.3.bn3.running_mean', 'backbone.0.layer2.3.bn3.running_var', 'backbone.0.layer2.3.bn3.num_batches_tracked', 'backbone.0.layer3.0.conv1.weight', 'backbone.0.layer3.0.bn1.weight', 'backbone.0.layer3.0.bn1.bias', 'backbone.0.layer3.0.bn1.running_mean', 'backbone.0.layer3.0.bn1.running_var', 'backbone.0.layer3.0.bn1.num_batches_tracked', 'backbone.0.layer3.0.conv2.weight', 'backbone.0.layer3.0.bn2.weight', 'backbone.0.layer3.0.bn2.bias', 'backbone.0.layer3.0.bn2.running_mean', 'backbone.0.layer3.0.bn2.running_var', 'backbone.0.layer3.0.bn2.num_batches_tracked', 'backbone.0.layer3.0.conv3.weight', 'backbone.0.layer3.0.bn3.weight', 'backbone.0.layer3.0.bn3.bias', 'backbone.0.layer3.0.bn3.running_mean', 'backbone.0.layer3.0.bn3.running_var', 'backbone.0.layer3.0.bn3.num_batches_tracked', 'backbone.0.layer3.0.downsample.0.weight', 'backbone.0.layer3.0.downsample.1.weight', 'backbone.0.layer3.0.downsample.1.bias', 'backbone.0.layer3.0.downsample.1.running_mean', 'backbone.0.layer3.0.downsample.1.running_var', 'backbone.0.layer3.0.downsample.1.num_batches_tracked', 'backbone.0.layer3.1.conv1.weight', 'backbone.0.layer3.1.bn1.weight', 'backbone.0.layer3.1.bn1.bias', 'backbone.0.layer3.1.bn1.running_mean', 'backbone.0.layer3.1.bn1.running_var', 'backbone.0.layer3.1.bn1.num_batches_tracked', 'backbone.0.layer3.1.conv2.weight', 'backbone.0.layer3.1.bn2.weight', 'backbone.0.layer3.1.bn2.bias', 'backbone.0.layer3.1.bn2.running_mean', 'backbone.0.layer3.1.bn2.running_var', 'backbone.0.layer3.1.bn2.num_batches_tracked', 'backbone.0.layer3.1.conv3.weight', 'backbone.0.layer3.1.bn3.weight', 'backbone.0.layer3.1.bn3.bias', 'backbone.0.layer3.1.bn3.running_mean', 'backbone.0.layer3.1.bn3.running_var', 'backbone.0.layer3.1.bn3.num_batches_tracked', 'backbone.0.layer3.2.conv1.weight', 'backbone.0.layer3.2.bn1.weight', 'backbone.0.layer3.2.bn1.bias', 'backbone.0.layer3.2.bn1.running_mean', 'backbone.0.layer3.2.bn1.running_var', 'backbone.0.layer3.2.bn1.num_batches_tracked', 'backbone.0.layer3.2.conv2.weight', 'backbone.0.layer3.2.bn2.weight', 'backbone.0.layer3.2.bn2.bias', 'backbone.0.layer3.2.bn2.running_mean', 'backbone.0.layer3.2.bn2.running_var', 'backbone.0.layer3.2.bn2.num_batches_tracked', 'backbone.0.layer3.2.conv3.weight', 'backbone.0.layer3.2.bn3.weight', 'backbone.0.layer3.2.bn3.bias', 'backbone.0.layer3.2.bn3.running_mean', 'backbone.0.layer3.2.bn3.running_var', 'backbone.0.layer3.2.bn3.num_batches_tracked', 'backbone.0.layer3.3.conv1.weight', 'backbone.0.layer3.3.bn1.weight', 'backbone.0.layer3.3.bn1.bias', 'backbone.0.layer3.3.bn1.running_mean', 'backbone.0.layer3.3.bn1.running_var', 'backbone.0.layer3.3.bn1.num_batches_tracked', 'backbone.0.layer3.3.conv2.weight', 'backbone.0.layer3.3.bn2.weight', 'backbone.0.layer3.3.bn2.bias', 'backbone.0.layer3.3.bn2.running_mean', 'backbone.0.layer3.3.bn2.running_var', 'backbone.0.layer3.3.bn2.num_batches_tracked', 'backbone.0.layer3.3.conv3.weight', 'backbone.0.layer3.3.bn3.weight', 'backbone.0.layer3.3.bn3.bias', 'backbone.0.layer3.3.bn3.running_mean', 'backbone.0.layer3.3.bn3.running_var', 'backbone.0.layer3.3.bn3.num_batches_tracked', 'backbone.0.layer3.4.conv1.weight', 'backbone.0.layer3.4.bn1.weight', 'backbone.0.layer3.4.bn1.bias', 'backbone.0.layer3.4.bn1.running_mean', 'backbone.0.layer3.4.bn1.running_var', 'backbone.0.layer3.4.bn1.num_batches_tracked', 'backbone.0.layer3.4.conv2.weight', 'backbone.0.layer3.4.bn2.weight', 'backbone.0.layer3.4.bn2.bias', 'backbone.0.layer3.4.bn2.running_mean', 'backbone.0.layer3.4.bn2.running_var', 'backbone.0.layer3.4.bn2.num_batches_tracked', 'backbone.0.layer3.4.conv3.weight', 'backbone.0.layer3.4.bn3.weight', 'backbone.0.layer3.4.bn3.bias', 'backbone.0.layer3.4.bn3.running_mean', 'backbone.0.layer3.4.bn3.running_var', 'backbone.0.layer3.4.bn3.num_batches_tracked', 'backbone.0.layer3.5.conv1.weight', 'backbone.0.layer3.5.bn1.weight', 'backbone.0.layer3.5.bn1.bias', 'backbone.0.layer3.5.bn1.running_mean', 'backbone.0.layer3.5.bn1.running_var', 'backbone.0.layer3.5.bn1.num_batches_tracked', 'backbone.0.layer3.5.conv2.weight', 'backbone.0.layer3.5.bn2.weight', 'backbone.0.layer3.5.bn2.bias', 'backbone.0.layer3.5.bn2.running_mean', 'backbone.0.layer3.5.bn2.running_var', 'backbone.0.layer3.5.bn2.num_batches_tracked', 'backbone.0.layer3.5.conv3.weight', 'backbone.0.layer3.5.bn3.weight', 'backbone.0.layer3.5.bn3.bias', 'backbone.0.layer3.5.bn3.running_mean', 'backbone.0.layer3.5.bn3.running_var', 'backbone.0.layer3.5.bn3.num_batches_tracked', 'backbone.0.layer4.0.conv1.weight', 'backbone.0.layer4.0.bn1.weight', 'backbone.0.layer4.0.bn1.bias', 'backbone.0.layer4.0.bn1.running_mean', 'backbone.0.layer4.0.bn1.running_var', 'backbone.0.layer4.0.bn1.num_batches_tracked', 'backbone.0.layer4.0.conv2.weight', 'backbone.0.layer4.0.bn2.weight', 'backbone.0.layer4.0.bn2.bias', 'backbone.0.layer4.0.bn2.running_mean', 'backbone.0.layer4.0.bn2.running_var', 'backbone.0.layer4.0.bn2.num_batches_tracked', 'backbone.0.layer4.0.conv3.weight', 'backbone.0.layer4.0.bn3.weight', 'backbone.0.layer4.0.bn3.bias', 'backbone.0.layer4.0.bn3.running_mean', 'backbone.0.layer4.0.bn3.running_var', 'backbone.0.layer4.0.bn3.num_batches_tracked', 'backbone.0.layer4.0.downsample.0.weight', 'backbone.0.layer4.0.downsample.1.weight', 'backbone.0.layer4.0.downsample.1.bias', 'backbone.0.layer4.0.downsample.1.running_mean', 'backbone.0.layer4.0.downsample.1.running_var', 'backbone.0.layer4.0.downsample.1.num_batches_tracked', 'backbone.0.layer4.1.conv1.weight', 'backbone.0.layer4.1.bn1.weight', 'backbone.0.layer4.1.bn1.bias', 'backbone.0.layer4.1.bn1.running_mean', 'backbone.0.layer4.1.bn1.running_var', 'backbone.0.layer4.1.bn1.num_batches_tracked', 'backbone.0.layer4.1.conv2.weight', 'backbone.0.layer4.1.bn2.weight', 'backbone.0.layer4.1.bn2.bias', 'backbone.0.layer4.1.bn2.running_mean', 'backbone.0.layer4.1.bn2.running_var', 'backbone.0.layer4.1.bn2.num_batches_tracked', 'backbone.0.layer4.1.conv3.weight', 'backbone.0.layer4.1.bn3.weight', 'backbone.0.layer4.1.bn3.bias', 'backbone.0.layer4.1.bn3.running_mean', 'backbone.0.layer4.1.bn3.running_var', 'backbone.0.layer4.1.bn3.num_batches_tracked', 'backbone.0.layer4.2.conv1.weight', 'backbone.0.layer4.2.bn1.weight', 'backbone.0.layer4.2.bn1.bias', 'backbone.0.layer4.2.bn1.running_mean', 'backbone.0.layer4.2.bn1.running_var', 'backbone.0.layer4.2.bn1.num_batches_tracked', 'backbone.0.layer4.2.conv2.weight', 'backbone.0.layer4.2.bn2.weight', 'backbone.0.layer4.2.bn2.bias', 'backbone.0.layer4.2.bn2.running_mean', 'backbone.0.layer4.2.bn2.running_var', 'backbone.0.layer4.2.bn2.num_batches_tracked', 'backbone.0.layer4.2.conv3.weight', 'backbone.0.layer4.2.bn3.weight', 'backbone.0.layer4.2.bn3.bias', 'backbone.0.layer4.2.bn3.running_mean', 'backbone.0.layer4.2.bn3.running_var', 'backbone.0.layer4.2.bn3.num_batches_tracked', 'backbone.0.deconv_layers.0.weight', 'backbone.0.deconv_layers.1.weight', 'backbone.0.deconv_layers.1.bias', 'backbone.0.deconv_layers.1.running_mean', 'backbone.0.deconv_layers.1.running_var', 'backbone.0.deconv_layers.1.num_batches_tracked', 'backbone.0.deconv_layers.3.weight', 'backbone.0.deconv_layers.4.weight', 'backbone.0.deconv_layers.4.bias', 'backbone.0.deconv_layers.4.running_mean', 'backbone.0.deconv_layers.4.running_var', 'backbone.0.deconv_layers.4.num_batches_tracked', 'backbone.0.deconv_layers.6.weight', 'backbone.0.deconv_layers.7.weight', 'backbone.0.deconv_layers.7.bias', 'backbone.0.deconv_layers.7.running_mean', 'backbone.0.deconv_layers.7.running_var', 'backbone.0.deconv_layers.7.num_batches_tracked', 'backbone.backbone.conv1.weight', 'backbone.backbone.bn1.weight', 'backbone.backbone.bn1.bias', 'backbone.backbone.bn1.running_mean', 'backbone.backbone.bn1.running_var', 'backbone.backbone.bn1.num_batches_tracked', 'backbone.backbone.layer1.0.conv1.weight', 'backbone.backbone.layer1.0.bn1.weight', 'backbone.backbone.layer1.0.bn1.bias', 'backbone.backbone.layer1.0.bn1.running_mean', 'backbone.backbone.layer1.0.bn1.running_var', 'backbone.backbone.layer1.0.bn1.num_batches_tracked', 'backbone.backbone.layer1.0.conv2.weight', 'backbone.backbone.layer1.0.bn2.weight', 'backbone.backbone.layer1.0.bn2.bias', 'backbone.backbone.layer1.0.bn2.running_mean', 'backbone.backbone.layer1.0.bn2.running_var', 'backbone.backbone.layer1.0.bn2.num_batches_tracked', 'backbone.backbone.layer1.0.conv3.weight', 'backbone.backbone.layer1.0.bn3.weight', 'backbone.backbone.layer1.0.bn3.bias', 'backbone.backbone.layer1.0.bn3.running_mean', 'backbone.backbone.layer1.0.bn3.running_var', 'backbone.backbone.layer1.0.bn3.num_batches_tracked', 'backbone.backbone.layer1.0.downsample.0.weight', 'backbone.backbone.layer1.0.downsample.1.weight', 'backbone.backbone.layer1.0.downsample.1.bias', 'backbone.backbone.layer1.0.downsample.1.running_mean', 'backbone.backbone.layer1.0.downsample.1.running_var', 'backbone.backbone.layer1.0.downsample.1.num_batches_tracked', 'backbone.backbone.layer1.1.conv1.weight', 'backbone.backbone.layer1.1.bn1.weight', 'backbone.backbone.layer1.1.bn1.bias', 'backbone.backbone.layer1.1.bn1.running_mean', 'backbone.backbone.layer1.1.bn1.running_var', 'backbone.backbone.layer1.1.bn1.num_batches_tracked', 'backbone.backbone.layer1.1.conv2.weight', 'backbone.backbone.layer1.1.bn2.weight', 'backbone.backbone.layer1.1.bn2.bias', 'backbone.backbone.layer1.1.bn2.running_mean', 'backbone.backbone.layer1.1.bn2.running_var', 'backbone.backbone.layer1.1.bn2.num_batches_tracked', 'backbone.backbone.layer1.1.conv3.weight', 'backbone.backbone.layer1.1.bn3.weight', 'backbone.backbone.layer1.1.bn3.bias', 'backbone.backbone.layer1.1.bn3.running_mean', 'backbone.backbone.layer1.1.bn3.running_var', 'backbone.backbone.layer1.1.bn3.num_batches_tracked', 'backbone.backbone.layer1.2.conv1.weight', 'backbone.backbone.layer1.2.bn1.weight', 'backbone.backbone.layer1.2.bn1.bias', 'backbone.backbone.layer1.2.bn1.running_mean', 'backbone.backbone.layer1.2.bn1.running_var', 'backbone.backbone.layer1.2.bn1.num_batches_tracked', 'backbone.backbone.layer1.2.conv2.weight', 'backbone.backbone.layer1.2.bn2.weight', 'backbone.backbone.layer1.2.bn2.bias', 'backbone.backbone.layer1.2.bn2.running_mean', 'backbone.backbone.layer1.2.bn2.running_var', 'backbone.backbone.layer1.2.bn2.num_batches_tracked', 'backbone.backbone.layer1.2.conv3.weight', 'backbone.backbone.layer1.2.bn3.weight', 'backbone.backbone.layer1.2.bn3.bias', 'backbone.backbone.layer1.2.bn3.running_mean', 'backbone.backbone.layer1.2.bn3.running_var', 'backbone.backbone.layer1.2.bn3.num_batches_tracked', 'backbone.backbone.layer2.0.conv1.weight', 'backbone.backbone.layer2.0.bn1.weight', 'backbone.backbone.layer2.0.bn1.bias', 'backbone.backbone.layer2.0.bn1.running_mean', 'backbone.backbone.layer2.0.bn1.running_var', 'backbone.backbone.layer2.0.bn1.num_batches_tracked', 'backbone.backbone.layer2.0.conv2.weight', 'backbone.backbone.layer2.0.bn2.weight', 'backbone.backbone.layer2.0.bn2.bias', 'backbone.backbone.layer2.0.bn2.running_mean', 'backbone.backbone.layer2.0.bn2.running_var', 'backbone.backbone.layer2.0.bn2.num_batches_tracked', 'backbone.backbone.layer2.0.conv3.weight', 'backbone.backbone.layer2.0.bn3.weight', 'backbone.backbone.layer2.0.bn3.bias', 'backbone.backbone.layer2.0.bn3.running_mean', 'backbone.backbone.layer2.0.bn3.running_var', 'backbone.backbone.layer2.0.bn3.num_batches_tracked', 'backbone.backbone.layer2.0.downsample.0.weight', 'backbone.backbone.layer2.0.downsample.1.weight', 'backbone.backbone.layer2.0.downsample.1.bias', 'backbone.backbone.layer2.0.downsample.1.running_mean', 'backbone.backbone.layer2.0.downsample.1.running_var', 'backbone.backbone.layer2.0.downsample.1.num_batches_tracked', 'backbone.backbone.layer2.1.conv1.weight', 'backbone.backbone.layer2.1.bn1.weight', 'backbone.backbone.layer2.1.bn1.bias', 'backbone.backbone.layer2.1.bn1.running_mean', 'backbone.backbone.layer2.1.bn1.running_var', 'backbone.backbone.layer2.1.bn1.num_batches_tracked', 'backbone.backbone.layer2.1.conv2.weight', 'backbone.backbone.layer2.1.bn2.weight', 'backbone.backbone.layer2.1.bn2.bias', 'backbone.backbone.layer2.1.bn2.running_mean', 'backbone.backbone.layer2.1.bn2.running_var', 'backbone.backbone.layer2.1.bn2.num_batches_tracked', 'backbone.backbone.layer2.1.conv3.weight', 'backbone.backbone.layer2.1.bn3.weight', 'backbone.backbone.layer2.1.bn3.bias', 'backbone.backbone.layer2.1.bn3.running_mean', 'backbone.backbone.layer2.1.bn3.running_var', 'backbone.backbone.layer2.1.bn3.num_batches_tracked', 'backbone.backbone.layer2.2.conv1.weight', 'backbone.backbone.layer2.2.bn1.weight', 'backbone.backbone.layer2.2.bn1.bias', 'backbone.backbone.layer2.2.bn1.running_mean', 'backbone.backbone.layer2.2.bn1.running_var', 'backbone.backbone.layer2.2.bn1.num_batches_tracked', 'backbone.backbone.layer2.2.conv2.weight', 'backbone.backbone.layer2.2.bn2.weight', 'backbone.backbone.layer2.2.bn2.bias', 'backbone.backbone.layer2.2.bn2.running_mean', 'backbone.backbone.layer2.2.bn2.running_var', 'backbone.backbone.layer2.2.bn2.num_batches_tracked', 'backbone.backbone.layer2.2.conv3.weight', 'backbone.backbone.layer2.2.bn3.weight', 'backbone.backbone.layer2.2.bn3.bias', 'backbone.backbone.layer2.2.bn3.running_mean', 'backbone.backbone.layer2.2.bn3.running_var', 'backbone.backbone.layer2.2.bn3.num_batches_tracked', 'backbone.backbone.layer2.3.conv1.weight', 'backbone.backbone.layer2.3.bn1.weight', 'backbone.backbone.layer2.3.bn1.bias', 'backbone.backbone.layer2.3.bn1.running_mean', 'backbone.backbone.layer2.3.bn1.running_var', 'backbone.backbone.layer2.3.bn1.num_batches_tracked', 'backbone.backbone.layer2.3.conv2.weight', 'backbone.backbone.layer2.3.bn2.weight', 'backbone.backbone.layer2.3.bn2.bias', 'backbone.backbone.layer2.3.bn2.running_mean', 'backbone.backbone.layer2.3.bn2.running_var', 'backbone.backbone.layer2.3.bn2.num_batches_tracked', 'backbone.backbone.layer2.3.conv3.weight', 'backbone.backbone.layer2.3.bn3.weight', 'backbone.backbone.layer2.3.bn3.bias', 'backbone.backbone.layer2.3.bn3.running_mean', 'backbone.backbone.layer2.3.bn3.running_var', 'backbone.backbone.layer2.3.bn3.num_batches_tracked', 'backbone.backbone.layer3.0.conv1.weight', 'backbone.backbone.layer3.0.bn1.weight', 'backbone.backbone.layer3.0.bn1.bias', 'backbone.backbone.layer3.0.bn1.running_mean', 'backbone.backbone.layer3.0.bn1.running_var', 'backbone.backbone.layer3.0.bn1.num_batches_tracked', 'backbone.backbone.layer3.0.conv2.weight', 'backbone.backbone.layer3.0.bn2.weight', 'backbone.backbone.layer3.0.bn2.bias', 'backbone.backbone.layer3.0.bn2.running_mean', 'backbone.backbone.layer3.0.bn2.running_var', 'backbone.backbone.layer3.0.bn2.num_batches_tracked', 'backbone.backbone.layer3.0.conv3.weight', 'backbone.backbone.layer3.0.bn3.weight', 'backbone.backbone.layer3.0.bn3.bias', 'backbone.backbone.layer3.0.bn3.running_mean', 'backbone.backbone.layer3.0.bn3.running_var', 'backbone.backbone.layer3.0.bn3.num_batches_tracked', 'backbone.backbone.layer3.0.downsample.0.weight', 'backbone.backbone.layer3.0.downsample.1.weight', 'backbone.backbone.layer3.0.downsample.1.bias', 'backbone.backbone.layer3.0.downsample.1.running_mean', 'backbone.backbone.layer3.0.downsample.1.running_var', 'backbone.backbone.layer3.0.downsample.1.num_batches_tracked', 'backbone.backbone.layer3.1.conv1.weight', 'backbone.backbone.layer3.1.bn1.weight', 'backbone.backbone.layer3.1.bn1.bias', 'backbone.backbone.layer3.1.bn1.running_mean', 'backbone.backbone.layer3.1.bn1.running_var', 'backbone.backbone.layer3.1.bn1.num_batches_tracked', 'backbone.backbone.layer3.1.conv2.weight', 'backbone.backbone.layer3.1.bn2.weight', 'backbone.backbone.layer3.1.bn2.bias', 'backbone.backbone.layer3.1.bn2.running_mean', 'backbone.backbone.layer3.1.bn2.running_var', 'backbone.backbone.layer3.1.bn2.num_batches_tracked', 'backbone.backbone.layer3.1.conv3.weight', 'backbone.backbone.layer3.1.bn3.weight', 'backbone.backbone.layer3.1.bn3.bias', 'backbone.backbone.layer3.1.bn3.running_mean', 'backbone.backbone.layer3.1.bn3.running_var', 'backbone.backbone.layer3.1.bn3.num_batches_tracked', 'backbone.backbone.layer3.2.conv1.weight', 'backbone.backbone.layer3.2.bn1.weight', 'backbone.backbone.layer3.2.bn1.bias', 'backbone.backbone.layer3.2.bn1.running_mean', 'backbone.backbone.layer3.2.bn1.running_var', 'backbone.backbone.layer3.2.bn1.num_batches_tracked', 'backbone.backbone.layer3.2.conv2.weight', 'backbone.backbone.layer3.2.bn2.weight', 'backbone.backbone.layer3.2.bn2.bias', 'backbone.backbone.layer3.2.bn2.running_mean', 'backbone.backbone.layer3.2.bn2.running_var', 'backbone.backbone.layer3.2.bn2.num_batches_tracked', 'backbone.backbone.layer3.2.conv3.weight', 'backbone.backbone.layer3.2.bn3.weight', 'backbone.backbone.layer3.2.bn3.bias', 'backbone.backbone.layer3.2.bn3.running_mean', 'backbone.backbone.layer3.2.bn3.running_var', 'backbone.backbone.layer3.2.bn3.num_batches_tracked', 'backbone.backbone.layer3.3.conv1.weight', 'backbone.backbone.layer3.3.bn1.weight', 'backbone.backbone.layer3.3.bn1.bias', 'backbone.backbone.layer3.3.bn1.running_mean', 'backbone.backbone.layer3.3.bn1.running_var', 'backbone.backbone.layer3.3.bn1.num_batches_tracked', 'backbone.backbone.layer3.3.conv2.weight', 'backbone.backbone.layer3.3.bn2.weight', 'backbone.backbone.layer3.3.bn2.bias', 'backbone.backbone.layer3.3.bn2.running_mean', 'backbone.backbone.layer3.3.bn2.running_var', 'backbone.backbone.layer3.3.bn2.num_batches_tracked', 'backbone.backbone.layer3.3.conv3.weight', 'backbone.backbone.layer3.3.bn3.weight', 'backbone.backbone.layer3.3.bn3.bias', 'backbone.backbone.layer3.3.bn3.running_mean', 'backbone.backbone.layer3.3.bn3.running_var', 'backbone.backbone.layer3.3.bn3.num_batches_tracked', 'backbone.backbone.layer3.4.conv1.weight', 'backbone.backbone.layer3.4.bn1.weight', 'backbone.backbone.layer3.4.bn1.bias', 'backbone.backbone.layer3.4.bn1.running_mean', 'backbone.backbone.layer3.4.bn1.running_var', 'backbone.backbone.layer3.4.bn1.num_batches_tracked', 'backbone.backbone.layer3.4.conv2.weight', 'backbone.backbone.layer3.4.bn2.weight', 'backbone.backbone.layer3.4.bn2.bias', 'backbone.backbone.layer3.4.bn2.running_mean', 'backbone.backbone.layer3.4.bn2.running_var', 'backbone.backbone.layer3.4.bn2.num_batches_tracked', 'backbone.backbone.layer3.4.conv3.weight', 'backbone.backbone.layer3.4.bn3.weight', 'backbone.backbone.layer3.4.bn3.bias', 'backbone.backbone.layer3.4.bn3.running_mean', 'backbone.backbone.layer3.4.bn3.running_var', 'backbone.backbone.layer3.4.bn3.num_batches_tracked', 'backbone.backbone.layer3.5.conv1.weight', 'backbone.backbone.layer3.5.bn1.weight', 'backbone.backbone.layer3.5.bn1.bias', 'backbone.backbone.layer3.5.bn1.running_mean', 'backbone.backbone.layer3.5.bn1.running_var', 'backbone.backbone.layer3.5.bn1.num_batches_tracked', 'backbone.backbone.layer3.5.conv2.weight', 'backbone.backbone.layer3.5.bn2.weight', 'backbone.backbone.layer3.5.bn2.bias', 'backbone.backbone.layer3.5.bn2.running_mean', 'backbone.backbone.layer3.5.bn2.running_var', 'backbone.backbone.layer3.5.bn2.num_batches_tracked', 'backbone.backbone.layer3.5.conv3.weight', 'backbone.backbone.layer3.5.bn3.weight', 'backbone.backbone.layer3.5.bn3.bias', 'backbone.backbone.layer3.5.bn3.running_mean', 'backbone.backbone.layer3.5.bn3.running_var', 'backbone.backbone.layer3.5.bn3.num_batches_tracked', 'backbone.backbone.layer4.0.conv1.weight', 'backbone.backbone.layer4.0.bn1.weight', 'backbone.backbone.layer4.0.bn1.bias', 'backbone.backbone.layer4.0.bn1.running_mean', 'backbone.backbone.layer4.0.bn1.running_var', 'backbone.backbone.layer4.0.bn1.num_batches_tracked', 'backbone.backbone.layer4.0.conv2.weight', 'backbone.backbone.layer4.0.bn2.weight', 'backbone.backbone.layer4.0.bn2.bias', 'backbone.backbone.layer4.0.bn2.running_mean', 'backbone.backbone.layer4.0.bn2.running_var', 'backbone.backbone.layer4.0.bn2.num_batches_tracked', 'backbone.backbone.layer4.0.conv3.weight', 'backbone.backbone.layer4.0.bn3.weight', 'backbone.backbone.layer4.0.bn3.bias', 'backbone.backbone.layer4.0.bn3.running_mean', 'backbone.backbone.layer4.0.bn3.running_var', 'backbone.backbone.layer4.0.bn3.num_batches_tracked', 'backbone.backbone.layer4.0.downsample.0.weight', 'backbone.backbone.layer4.0.downsample.1.weight', 'backbone.backbone.layer4.0.downsample.1.bias', 'backbone.backbone.layer4.0.downsample.1.running_mean', 'backbone.backbone.layer4.0.downsample.1.running_var', 'backbone.backbone.layer4.0.downsample.1.num_batches_tracked', 'backbone.backbone.layer4.1.conv1.weight', 'backbone.backbone.layer4.1.bn1.weight', 'backbone.backbone.layer4.1.bn1.bias', 'backbone.backbone.layer4.1.bn1.running_mean', 'backbone.backbone.layer4.1.bn1.running_var', 'backbone.backbone.layer4.1.bn1.num_batches_tracked', 'backbone.backbone.layer4.1.conv2.weight', 'backbone.backbone.layer4.1.bn2.weight', 'backbone.backbone.layer4.1.bn2.bias', 'backbone.backbone.layer4.1.bn2.running_mean', 'backbone.backbone.layer4.1.bn2.running_var', 'backbone.backbone.layer4.1.bn2.num_batches_tracked', 'backbone.backbone.layer4.1.conv3.weight', 'backbone.backbone.layer4.1.bn3.weight', 'backbone.backbone.layer4.1.bn3.bias', 'backbone.backbone.layer4.1.bn3.running_mean', 'backbone.backbone.layer4.1.bn3.running_var', 'backbone.backbone.layer4.1.bn3.num_batches_tracked', 'backbone.backbone.layer4.2.conv1.weight', 'backbone.backbone.layer4.2.bn1.weight', 'backbone.backbone.layer4.2.bn1.bias', 'backbone.backbone.layer4.2.bn1.running_mean', 'backbone.backbone.layer4.2.bn1.running_var', 'backbone.backbone.layer4.2.bn1.num_batches_tracked', 'backbone.backbone.layer4.2.conv2.weight', 'backbone.backbone.layer4.2.bn2.weight', 'backbone.backbone.layer4.2.bn2.bias', 'backbone.backbone.layer4.2.bn2.running_mean', 'backbone.backbone.layer4.2.bn2.running_var', 'backbone.backbone.layer4.2.bn2.num_batches_tracked', 'backbone.backbone.layer4.2.conv3.weight', 'backbone.backbone.layer4.2.bn3.weight', 'backbone.backbone.layer4.2.bn3.bias', 'backbone.backbone.layer4.2.bn3.running_mean', 'backbone.backbone.layer4.2.bn3.running_var', 'backbone.backbone.layer4.2.bn3.num_batches_tracked', 'backbone.backbone.deconv_layers.0.weight', 'backbone.backbone.deconv_layers.1.weight', 'backbone.backbone.deconv_layers.1.bias', 'backbone.backbone.deconv_layers.1.running_mean', 'backbone.backbone.deconv_layers.1.running_var', 'backbone.backbone.deconv_layers.1.num_batches_tracked', 'backbone.backbone.deconv_layers.3.weight', 'backbone.backbone.deconv_layers.4.weight', 'backbone.backbone.deconv_layers.4.bias', 'backbone.backbone.deconv_layers.4.running_mean', 'backbone.backbone.deconv_layers.4.running_var', 'backbone.backbone.deconv_layers.4.num_batches_tracked', 'backbone.backbone.deconv_layers.6.weight', 'backbone.backbone.deconv_layers.7.weight', 'backbone.backbone.deconv_layers.7.bias', 'backbone.backbone.deconv_layers.7.running_mean', 'backbone.backbone.deconv_layers.7.running_var', 'backbone.backbone.deconv_layers.7.num_batches_tracked', 'position_embedding.row_embed.weight', 'position_embedding.col_embed.weight', '_result_parser.params_map_parser.smpl_model.betas', '_result_parser.params_map_parser.smpl_model.faces_tensor', '_result_parser.params_map_parser.smpl_model.v_template', '_result_parser.params_map_parser.smpl_model.shapedirs', '_result_parser.params_map_parser.smpl_model.J_regressor', '_result_parser.params_map_parser.smpl_model.J_regressor_extra9', '_result_parser.params_map_parser.smpl_model.J_regressor_h36m17', '_result_parser.params_map_parser.smpl_model.posedirs', '_result_parser.params_map_parser.smpl_model.parents', '_result_parser.params_map_parser.smpl_model.lbs_weights', '_result_parser.params_map_parser.smpl_model.vertex_joint_selector.extra_joints_idxs', 'transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.out_proj.weight', 'transformer.decoder.layers.4.self_attn.out_proj.bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'transformer.decoder.layers.4.linear1.weight', 'transformer.decoder.layers.4.linear1.bias', 'transformer.decoder.layers.4.linear2.weight', 'transformer.decoder.layers.4.linear2.bias', 'transformer.decoder.layers.4.norm1.weight', 'transformer.decoder.layers.4.norm1.bias', 'transformer.decoder.layers.4.norm2.weight', 'transformer.decoder.layers.4.norm2.bias', 'transformer.decoder.layers.4.norm3.weight', 'transformer.decoder.layers.4.norm3.bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.out_proj.weight', 'transformer.decoder.layers.5.self_attn.out_proj.bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'transformer.decoder.layers.5.linear1.weight', 'transformer.decoder.layers.5.linear1.bias', 'transformer.decoder.layers.5.linear2.weight', 'transformer.decoder.layers.5.linear2.bias', 'transformer.decoder.layers.5.norm1.weight', 'transformer.decoder.layers.5.norm1.bias', 'transformer.decoder.layers.5.norm2.weight', 'transformer.decoder.layers.5.norm2.bias', 'transformer.decoder.layers.5.norm3.weight', 'transformer.decoder.layers.5.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'input_proj.weight', 'input_proj.bias', 'output_proj1.weight', 'output_proj1.bias', 'output_proj2.weight', 'output_proj2.bias', 'output_proj3.weight', 'output_proj3.bias']\n",
      "INFO:root:Train all layers, except: ['_result_parser.params_map_parser.smpl_model.betas']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization finished!\n",
      "Processing demo/images, saving to demo/image_results2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:gathering datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3 images to process\n",
      "torch.Size([3, 512, 512, 3]) torch.Size([3, 512, 512, 3])\n",
      "{'mode': 'parsing', 'calc_loss': False}\n",
      "output\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m processor \u001b[38;5;241m=\u001b[39m Image_processor(args_set\u001b[38;5;241m=\u001b[39margs())\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m args()\u001b[38;5;241m.\u001b[39minputs\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/predict/image.py:83\u001b[0m, in \u001b[0;36mImage_processor.run\u001b[0;34m(self, image_folder, tracker)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# plt.imshow(meta_data['image'][0].permute(1,2, 0), cmap = \"gray\")\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# plt.savefig(\"test.png\")\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemo_cfg)\n\u001b[0;32m---> 83\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_meta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdemo_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mkeys(),\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     86\u001b[0m reorganize_idx \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreorganize_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/predict/base_predictor.py:26\u001b[0m, in \u001b[0;36mPredictor.net_forward\u001b[0;34m(self, meta_data, window_meta_data, cfg)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_precision\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 26\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_meta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(meta_data, window_meta_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n",
      "File \u001b[0;32m/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/base.py:32\u001b[0m, in \u001b[0;36mBase.forward\u001b[0;34m(self, meta_data, window_meta_data, **cfg)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatching_forward(meta_data, window_meta_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsing\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparsing_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_meta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpure_forward(meta_data, window_meta_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg)\n",
      "File \u001b[0;32m/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/base.py:59\u001b[0m, in \u001b[0;36mBase.parsing_forward\u001b[0;34m(self, meta_data, window_meta_data, **cfg)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args()\u001b[38;5;241m.\u001b[39mmodel_precision\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 59\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_meta_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m         outputs, meta_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_parser\u001b[38;5;241m.\u001b[39mparsing_forward(outputs, meta_data, cfg)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/base.py:71\u001b[0m, in \u001b[0;36mBase.feed_forward\u001b[0;34m(self, meta_data, window_meta_data)\u001b[0m\n\u001b[1;32m     69\u001b[0m meta_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m meta_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     70\u001b[0m window_meta_data \u001b[38;5;241m=\u001b[39m window_meta_data\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_meta_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/modelv1.py:91\u001b[0m, in \u001b[0;36mHOBJ.head_forward\u001b[0;34m(self, meta_data, window_meta_data)\u001b[0m\n\u001b[1;32m     87\u001b[0m window_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(window_src)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# print(src.shape,window_src.shape,pos.shape,window_pos.shape,333)   \u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m hs, hs_without_norm, memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m center_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj1(hs)\n\u001b[1;32m     94\u001b[0m cam_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj2(hs)\n",
      "File \u001b[0;32m/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/transformer.py:61\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, pos_embed, window_src, window_pos_embed)\u001b[0m\n\u001b[1;32m     58\u001b[0m pos_embed_encoder \u001b[38;5;241m=\u001b[39m window_pos_embed\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (HW,N,C)  \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# print(src.shape,window_src.shape,pos_embed_encoder.shape,pos_embed_decoder.shape,11) \u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_embed_encoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# print(memory.shape,'enc_output')\u001b[39;00m\n\u001b[1;32m     64\u001b[0m hs, hs_without_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(src, memory, memory_key_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m                                    pos\u001b[38;5;241m=\u001b[39mpos_embed_decoder,encoder_pos\u001b[38;5;241m=\u001b[39mpos_embed_encoder)\n",
      "File \u001b[0;32m/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/transformer.py:91\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m     88\u001b[0m output \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m/z/home/mahzad-khosh/env/romp2/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/transformer.py:160\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_before:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_pre(src, src_mask, src_key_padding_mask, pos)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/transformer.py:127\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward_post\u001b[0;34m(self, src, src_mask, src_key_padding_mask, pos)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_post\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m                  src,\n\u001b[1;32m    124\u001b[0m                  src_mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m                  src_key_padding_mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m                  pos: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 127\u001b[0m     q \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_pos_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     src2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(q, k, value\u001b[38;5;241m=\u001b[39msrc, attn_mask\u001b[38;5;241m=\u001b[39msrc_mask,\n\u001b[1;32m    130\u001b[0m                           key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    132\u001b[0m     src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(src2)\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/transformer.py:120\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.with_pos_embed\u001b[0;34m(self, tensor, pos)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_pos_embed\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, pos: Optional[Tensor]):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# second, run the code\n",
    "processor = Image_processor(args_set=args())\n",
    "inputs = args().inputs\n",
    "results = processor.run(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608e794-0459-4f61-a37a-8f3214867c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import glob\n",
    "for img_path in glob.glob('demo/image_results2/*.jpg'):\n",
    "    display(Image(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b3903-a6ac-47e3-8c37-963c7cf31c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d7e2d-6a62-4e3a-a527-ba7d89db1637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566db61-881a-4d47-a9bb-c870ab25c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, if you want to export the model to ONNX:\n",
    "# torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa920d7-5e98-4ac2-a74a-9b8828a3847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models as py_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc90a561-58ef-4864-94d8-11b5b45b9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HumanObj_videos_ResNet.lib.models.BackboneWithFPN import _resnet_fpn_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "260a7099-5ae0-48eb-aca0-eb15ef3964b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = py_models.resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837bcf50-8c53-4da8-b47d-a67fda3d9b2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'norm_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43m_resnet_fpn_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/BackboneWithFPN.py:147\u001b[0m, in \u001b[0;36m_resnet_fpn_extractor\u001b[0;34m(backbone, trainable_layers, returned_layers, extra_blocks, norm_layer)\u001b[0m\n\u001b[1;32m    145\u001b[0m in_channels_list \u001b[38;5;241m=\u001b[39m [in_channels_stage2 \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m returned_layers]\n\u001b[1;32m    146\u001b[0m out_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackboneWithFPN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/z/home/mkhoshle/Human_object_transform/HumanObj_videos_ResNet/lib/models/BackboneWithFPN.py:46\u001b[0m, in \u001b[0;36mBackboneWithFPN.__init__\u001b[0;34m(self, backbone, return_layers, in_channels_list, out_channels, extra_blocks, norm_layer)\u001b[0m\n\u001b[1;32m     43\u001b[0m     extra_blocks \u001b[38;5;241m=\u001b[39m LastLevelMaxPool()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m IntermediateLayerGetter(backbone, return_layers\u001b[38;5;241m=\u001b[39mreturn_layers)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpn \u001b[38;5;241m=\u001b[39m \u001b[43mFeaturePyramidNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels \u001b[38;5;241m=\u001b[39m out_channels\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'norm_layer'"
     ]
    }
   ],
   "source": [
    "out = _resnet_fpn_extractor(backbone, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f574e82-a82f-4e94-b9d6-5300c6173b23",
   "metadata": {},
   "source": [
    "## Plot Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b15c62e-579b-40e2-849e-98e5f0f9ab9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Running</td>\n",
       "      <td>MPJPE:327.5680236816406|0.0;</td>\n",
       "      <td>Running</td>\n",
       "      <td>PAMPJPE:212.51625061035156|0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Running</td>\n",
       "      <td>MPJPE:306.97698974609375|423.9913330078125;</td>\n",
       "      <td>Running</td>\n",
       "      <td>PAMPJPE:197.36795043945312|229.4709930419922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Running</td>\n",
       "      <td>MPJPE:289.9775390625|860.6228637695312;</td>\n",
       "      <td>Running</td>\n",
       "      <td>PAMPJPE:183.23876953125|552.2484741210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Running</td>\n",
       "      <td>MPJPE:268.9407958984375|1973.099609375;</td>\n",
       "      <td>Running</td>\n",
       "      <td>PAMPJPE:172.0282745361328|791.2120971679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Running</td>\n",
       "      <td>MPJPE:253.4237823486328|2541.59228515625;</td>\n",
       "      <td>Running</td>\n",
       "      <td>PAMPJPE:163.92245483398438|895.7868041992188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                            1        2  \\\n",
       "0  Running                 MPJPE:327.5680236816406|0.0;  Running   \n",
       "1  Running  MPJPE:306.97698974609375|423.9913330078125;  Running   \n",
       "2  Running      MPJPE:289.9775390625|860.6228637695312;  Running   \n",
       "3  Running      MPJPE:268.9407958984375|1973.099609375;  Running   \n",
       "4  Running    MPJPE:253.4237823486328|2541.59228515625;  Running   \n",
       "\n",
       "                                              3  \n",
       "0                PAMPJPE:212.51625061035156|0.0  \n",
       "1  PAMPJPE:197.36795043945312|229.4709930419922  \n",
       "2     PAMPJPE:183.23876953125|552.2484741210938  \n",
       "3   PAMPJPE:172.0282745361328|791.2120971679688  \n",
       "4  PAMPJPE:163.92245483398438|895.7868041992188  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"val.txt\"\n",
    "df_val = pd.read_csv(file_path, sep=\" \", header=None)\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6654a5d3-acf0-4918-a541-1d696bcd18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x.rstrip(';')\n",
    "df_val[[1]] = df_val[[1]].applymap(f)\n",
    "    \n",
    "f = lambda x: x.replace(\"MPJPE:\", \"\")\n",
    "df_val[[1]] = df_val[[1]].applymap(f)\n",
    "\n",
    "df_val[[\"MPJPE_mean\",\"MPJPE_var\"]] = df_val[1].str.split(\"|\", expand = True)\n",
    "\n",
    "f = lambda x: float(x)\n",
    "df_val[[\"MPJPE_mean\",\"MPJPE_var\"]] = df_val[[\"MPJPE_mean\",\"MPJPE_var\"]].applymap(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbebb563-0113-4ad5-83bd-4754a89459c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x.rstrip(';')\n",
    "df_val[[3]] = df_val[[3]].applymap(f)\n",
    "    \n",
    "f = lambda x: x.replace(\"PAMPJPE:\", \"\")\n",
    "df_val[[3]] = df_val[[3]].applymap(f)\n",
    "\n",
    "df_val[[\"PAMPJPE_mean\",\"PAMPJPE_var\"]] = df_val[3].str.split(\"|\", expand = True)\n",
    "\n",
    "f = lambda x: float(x)\n",
    "df_val[[\"PAMPJPE_mean\",\"PAMPJPE_var\"]] = df_val[[\"PAMPJPE_mean\",\"PAMPJPE_var\"]].applymap(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75072630-161a-47db-b2b6-1b70d080b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val.drop([0,1,2,3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0249f133-c32d-451f-83d7-7cec240f1fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['step'] = [i*2000 for i in range(len(df_val))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faf91ef1-1be6-43ee-820e-14fcfcfcc859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPJPE_mean</th>\n",
       "      <th>MPJPE_var</th>\n",
       "      <th>PAMPJPE_mean</th>\n",
       "      <th>PAMPJPE_var</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>327.568024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>212.516251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>306.976990</td>\n",
       "      <td>423.991333</td>\n",
       "      <td>197.367950</td>\n",
       "      <td>229.470993</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>289.977539</td>\n",
       "      <td>860.622864</td>\n",
       "      <td>183.238770</td>\n",
       "      <td>552.248474</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>268.940796</td>\n",
       "      <td>1973.099609</td>\n",
       "      <td>172.028275</td>\n",
       "      <td>791.212097</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253.423782</td>\n",
       "      <td>2541.592285</td>\n",
       "      <td>163.922455</td>\n",
       "      <td>895.786804</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MPJPE_mean    MPJPE_var  PAMPJPE_mean  PAMPJPE_var  step\n",
       "0  327.568024     0.000000    212.516251     0.000000     0\n",
       "1  306.976990   423.991333    197.367950   229.470993  2000\n",
       "2  289.977539   860.622864    183.238770   552.248474  4000\n",
       "3  268.940796  1973.099609    172.028275   791.212097  6000\n",
       "4  253.423782  2541.592285    163.922455   895.786804  8000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff48d8c9-1934-4f51-a0cf-1c8d80331f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['font.size'] = 9\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "axes = fig.add_subplot(1,1,1)\n",
    "\n",
    "yerrs = [np.sqrt(df_val['MPJPE_var'].values.transpose().tolist()),np.sqrt(df_val['PAMPJPE_var'].values.transpose().tolist())]\n",
    "\n",
    "df_val.plot(x='step', y=['PAMPJPE_mean','MPJPE_mean'], color=['blue','red'], yerr=yerrs, ax=axes) \n",
    "\n",
    "# df_val.loc[:,['PAMPJPE_mean','MPJPE_mean']].plot(color=['blue','red'],yerr=yerrs,ax=axes) \n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('ticks')\n",
    "axes.set_xlabel('$Iterations$')   \n",
    "axes.set_ylabel('Performance')\n",
    "\n",
    "axes.set_ylim(0, 400)\n",
    "# axes.set_xlim(0, 220)\n",
    "axes.yaxis.grid(which='major', linewidth='0.5', color='grey')\n",
    "axes.xaxis.grid(which='major', linewidth='0.5', color='grey')\n",
    "axes.yaxis.grid(which='minor', linewidth='0.5', color='grey')\n",
    "axes.xaxis.grid(which='minor', linewidth='0.5', color='grey') \n",
    "sns.despine(offset=10, ax=axes)\n",
    "\n",
    "# lgd = plt.legend(['IO','Compute','Opening Trajectory','Communication'],loc='upper center', bbox_to_anchor=(0.43, -0.38),\n",
    "                 # frameon=False, shadow=False, ncol=2, prop={'size':12})\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(bottom=0.4)\n",
    "\n",
    "fig.savefig('{}.pdf'.format('validation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447165b-a7bf-4a6e-9668-6ecbe8c69e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8935a41-5b1c-4d40-858b-7dd73c0fc406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
