{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7393510f-1b65-443c-878a-31dbe846864a",
   "metadata": {},
   "source": [
    "- Handling the case of video vs a single image for transformer input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44920a9b-3501-4a0e-a737-544cdd3fd223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/Arthur151/ROMP/releases/download/v1.1/model_data.zip\n",
    "# !unzip model_data.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab9c00-c270-4d5c-a0f0-e00f4603a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(,project_dir,source_dir,root_dir,model_dir,trained_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c263180-7eea-4e8e-852a-934ee8f4fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import os.path as op\n",
    "import code\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38304513-1e9b-4430-b7bf-6513548a18fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models.backbone import build_backbone\n",
    "from dataset.mixed_dataset import MixedDataset, SingleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335cd8ef-10a4-4336-beda-a4f82465b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "#########################################################\n",
    "# Data related arguments\n",
    "#########################################################\n",
    "parser.add_argument(\"--data_dir\", default='datasets', type=str, required=False,\n",
    "                    help=\"Directory with all datasets, each in one subfolder\")\n",
    "parser.add_argument(\"--train_yaml\", default='imagenet2012/train.yaml', type=str, required=False,\n",
    "                    help=\"Yaml file with all data for training.\")\n",
    "parser.add_argument(\"--val_yaml\", default='imagenet2012/test.yaml', type=str, required=False,\n",
    "                    help=\"Yaml file with all data for validation.\")\n",
    "parser.add_argument(\"--num_workers\", default=4, type=int, \n",
    "                    help=\"Workers in dataloader.\")       \n",
    "parser.add_argument(\"--img_scale_factor\", default=1, type=int, \n",
    "                    help=\"adjust image resolution.\")  \n",
    "#########################################################\n",
    "# Loading/saving checkpoints\n",
    "#########################################################\n",
    "parser.add_argument(\"--model_name_or_path\", default='metro/modeling/bert/bert-base-uncased/', type=str, required=False,\n",
    "                    help=\"Path to pre-trained transformer model or model type.\")\n",
    "parser.add_argument(\"--resume_checkpoint\", default=None, type=str, required=False,\n",
    "                    help=\"Path to specific checkpoint for resume training.\")\n",
    "parser.add_argument(\"--output_dir\", default='output/', type=str, required=False,\n",
    "                    help=\"The output directory to save checkpoint and test results.\")\n",
    "parser.add_argument(\"--config_name\", default=\"\", type=str, \n",
    "                    help=\"Pretrained config name or path if not the same as model_name.\")\n",
    "parser.add_argument('--backbone', default='resnet101',\n",
    "                help='CNN backbone architecture: hrnet-w64, hrnet, resnet50')\n",
    "#########################################################\n",
    "# Training parameters\n",
    "#########################################################\n",
    "parser.add_argument(\"--per_gpu_train_batch_size\", default=64, type=int, \n",
    "                    help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--per_gpu_eval_batch_size\", default=64, type=int, \n",
    "                    help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "parser.add_argument('--lr', \"--learning_rate\", default=1e-4, type=float, \n",
    "                    help=\"The initial lr.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=200, type=int, \n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--vertices_loss_weight\", default=1.0, type=float)          \n",
    "parser.add_argument(\"--joints_loss_weight\", default=1.0, type=float)\n",
    "parser.add_argument(\"--drop_out\", default=0.1, type=float, \n",
    "                    help=\"Drop out ratio in BERT.\")\n",
    "#########################################################\n",
    "# Model architectures\n",
    "#########################################################\n",
    "parser.add_argument(\"--num_hidden_layers\", default=-1, type=int, required=False, \n",
    "                    help=\"Update model config if given\")\n",
    "parser.add_argument(\"--hidden_size\", default=-1, type=int, required=False, \n",
    "                    help=\"Update model config if given\")\n",
    "parser.add_argument(\"--num_attention_heads\", default=-1, type=int, required=False, \n",
    "                    help=\"Update model config if given. Note that the division of \"\n",
    "                    \"hidden_size / num_attention_heads should be in integer.\")\n",
    " \n",
    "#########################################################\n",
    "# Others\n",
    "#########################################################\n",
    "parser.add_argument(\"--run_eval_only\", default=False, action='store_true',) \n",
    "\n",
    "parser.add_argument('--logging_steps', type=int, default=100, \n",
    "                    help=\"Log every X steps.\")\n",
    "parser.add_argument(\"--device\", type=str, default='cuda', \n",
    "                    help=\"cuda or cpu\")\n",
    "parser.add_argument('--seed', type=int, default=88, \n",
    "                    help=\"random seed for initialization.\")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0, \n",
    "                    help=\"For distributed training.\")\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=256, \n",
    "                    help=\"Size of the embeddings (dimension of the transformer.\")\n",
    "parser.add_argument(\"--position_embedding\", type=str, default='sine', \n",
    "                    help=\"Type of positional embedding to use on top \\\n",
    "                    of the image features. (sine, learned).\")\n",
    "parser.add_argument(\"--lr_backbone\", type=float, default=0.00001, \n",
    "                    help=\"Learning rate for backbone.\")\n",
    "parser.add_argument(\"--masks\", type=bool, default=False, \n",
    "                    help=\"Segmentation\")\n",
    "parser.add_argument(\"--num_feature_levels\", type=int, default=1, \n",
    "                    help=\"Number of feature levels the encoder processes from the backbone\")\n",
    "parser.add_argument(\"--dilation\", type=bool, default=False, \n",
    "                    help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f82c0-2704-43da-96ba-f252eb6d1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a3ff5-cc92-4d1e-ad29-6ae6eb0bfd1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _create_single_data_loader(datasets, shuffle=False, **kwargs):\n",
    "    datasets = SingleDataset(dataset=datasets, **kwargs)\n",
    "    return DataLoader(dataset = datasets, shuffle=shuffle,batch_size = args.per_gpu_train_batch_size,\\\n",
    "            drop_last = False, pin_memory = True, num_workers = 2)\n",
    "\n",
    "_create_single_data_loader('coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848e222-896f-4f03-8783-6c52d92cf47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = build_backbone(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1f7bc-148c-4db0-8883-3032f6c42774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Base):\n",
    "    def __init__(self):\n",
    "        super(Trainer, self).__init__()\n",
    "        self._build_model_()\n",
    "        self._build_optimizer()\n",
    "        self.set_up_val_loader()\n",
    "        self._calc_loss = Loss()\n",
    "        self.loader = self._create_data_loader(train_flag=True)\n",
    "        self.mutli_task_uncertainty_weighted_loss = Learnable_Loss(self.loader.dataset._get_ID_num_()).cuda()\n",
    "        self.optimizer.add_param_group({'params': self.mutli_task_uncertainty_weighted_loss.parameters()})\n",
    "        \n",
    "        self.train_cfg = {'mode':'matching_gts', 'is_training':True, 'update_data': True, 'calc_loss': True if self.model_return_loss else False, \\\n",
    "                           'new_training': args().new_training}\n",
    "        self.val_best_PAMPJPE = {'pw3d': 60, 'mpiinf':80}\n",
    "        logging.info('Initialization of Trainer finished!')\n",
    "\n",
    "    def train(self):\n",
    "        #init_seeds(self.local_rank, cuda_deterministic=False)\n",
    "        logging.info('start training')\n",
    "        self.model.train()\n",
    "        if self.fix_backbone_training_scratch:\n",
    "            fix_backbone(self.model, exclude_key=['backbone.'])\n",
    "        else:\n",
    "            train_entire_model(self.model)\n",
    "        for epoch in range(self.epoch):\n",
    "            if epoch==1:\n",
    "                train_entire_model(self.model)\n",
    "            self.train_epoch(epoch)\n",
    "        self.summary_writer.close()\n",
    "\n",
    "    def train_step(self, meta_data):\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.network_forward(self.model, meta_data, self.train_cfg)\n",
    "        \n",
    "        if not self.model_return_loss:\n",
    "            outputs.update(self._calc_loss(outputs))\n",
    "        loss, outputs = self.mutli_task_uncertainty_weighted_loss(outputs)\n",
    "\n",
    "        if self.model_precision=='fp16':\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return outputs, loss\n",
    "\n",
    "    def train_log_visualization(self, outputs, loss, run_time, data_time, losses, losses_dict, epoch, iter_index):\n",
    "        losses.update(loss.item())\n",
    "        losses_dict.update(outputs['loss_dict'])\n",
    "        if self.global_count%self.print_freq==0:\n",
    "            message = 'Epoch: [{0}][{1}/{2}] Time {data_time.avg:.2f} RUN {run_time.avg:.2f} Lr {lr} Loss {loss.avg:.2f} | Losses {3}'.format(\n",
    "                      epoch, iter_index + 1,  len(self.loader), losses_dict.avg(), #Acc {3} | accuracies.avg(), \n",
    "                      data_time=data_time, run_time=run_time, loss=losses, lr = self.optimizer.param_groups[0]['lr'])\n",
    "            print(message)\n",
    "            write2log(self.log_file,'%s\\n' % message)\n",
    "            self.summary_writer.add_scalar('loss', losses.avg, self.global_count)\n",
    "            self.summary_writer.add_scalars('loss_items', losses_dict.avg(), self.global_count)\n",
    "            \n",
    "            losses.reset(); losses_dict.reset(); data_time.reset() #accuracies.reset(); \n",
    "            self.summary_writer.flush()\n",
    "\n",
    "        if self.global_count%(6*self.print_freq)==0 or self.global_count==50:\n",
    "            vis_ids, vis_errors = determ_worst_best(outputs['kp_error'], top_n=3)\n",
    "            save_name = '{}'.format(self.global_count)\n",
    "            for ds_name in set(outputs['meta_data']['data_set']):\n",
    "                save_name += '_{}'.format(ds_name)\n",
    "            train_vis_dict = self.visualizer.visulize_result(outputs, outputs['meta_data'], show_items=['org_img', 'mesh', 'pj2d', 'centermap'],\\\n",
    "                vis_cfg={'settings': ['save_img'], 'vids': vis_ids, 'save_dir':self.train_img_dir, 'save_name':save_name, 'verrors': [vis_errors], 'error_names':['E']})\n",
    "\n",
    "    def train_epoch(self,epoch):\n",
    "        run_time, data_time, losses = [AverageMeter() for i in range(3)]\n",
    "        losses_dict= AverageMeter_Dict()\n",
    "        batch_start_time = time.time()\n",
    "        for iter_index, meta_data in enumerate(self.loader):\n",
    "            #torch.cuda.reset_peak_memory_stats(device=0)\n",
    "            self.global_count += 1\n",
    "            if args().new_training:\n",
    "                if self.global_count==args().new_training_iters:\n",
    "                    self.train_cfg['new_training'],self.val_cfg['new_training'],self.eval_cfg['new_training'] = False, False, False\n",
    "\n",
    "            data_time.update(time.time() - batch_start_time)\n",
    "            run_start_time = time.time()\n",
    "\n",
    "            outputs, loss = self.train_step(meta_data)\n",
    "\n",
    "            if self.local_rank in [-1, 0]:\n",
    "                run_time.update(time.time() - run_start_time)\n",
    "                self.train_log_visualization(outputs, loss, run_time, data_time, losses, losses_dict, epoch, iter_index)\n",
    "            \n",
    "            if self.global_count%self.test_interval==0 or self.global_count==self.fast_eval_iter: #self.print_freq*2\n",
    "                save_model(self.model,'{}_val_cache.pkl'.format(self.tab),parent_folder=self.model_save_dir)\n",
    "                self.validation(epoch)\n",
    "            \n",
    "            if self.distributed_training:\n",
    "                # wait for rank 0 process finish the job\n",
    "                torch.distributed.barrier()\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "        title  = '{}_epoch_{}.pkl'.format(self.tab,epoch)\n",
    "        save_model(self.model,title,parent_folder=self.model_save_dir)\n",
    "        self.e_sche.step()\n",
    "\n",
    "    def validation(self,epoch):\n",
    "        logging.info('evaluation result on {} iters: '.format(epoch))\n",
    "        for ds_name, val_loader in self.dataset_val_list.items():\n",
    "            logging.info('Evaluation on {}'.format(ds_name))\n",
    "            MPJPE, PA_MPJPE, eval_results = val_result(self,loader_val=val_loader, evaluation=False)\n",
    "            if ds_name=='pw3d' and PA_MPJPE<self.val_best_PAMPJPE['pw3d']:\n",
    "                self.val_best_PAMPJPE['pw3d'] = PA_MPJPE\n",
    "                _, _, eval_results = val_result(self,loader_val=self.dataset_test_list['pw3d'], evaluation=True)\n",
    "                self.summary_writer.add_scalars('pw3d-vibe-test', eval_results, self.global_count)\n",
    "            if ds_name=='mpiinf':\n",
    "                _, _, eval_results = val_result(self,loader_val=self.dataset_test_list['mpiinf'], evaluation=True)\n",
    "                self.summary_writer.add_scalars('mpiinf-test', eval_results, self.global_count)\n",
    "   \n",
    "            self.evaluation_results_dict[ds_name]['MPJPE'].append(MPJPE)\n",
    "            self.evaluation_results_dict[ds_name]['PAMPJPE'].append(PA_MPJPE)\n",
    "\n",
    "            logging.info('Running evaluation results:')\n",
    "            ds_running_results = self.get_running_results(ds_name)\n",
    "            print('Running MPJPE:{}|{}; Running PAMPJPE:{}|{}'.format(*ds_running_results))\n",
    "\n",
    "        title = '{}_{:.4f}_{:.4f}_{}.pkl'.format(epoch, MPJPE, PA_MPJPE, self.tab)\n",
    "        logging.info('Model saved as {}'.format(title))\n",
    "        save_model(self.model,title,parent_folder=self.model_save_dir)\n",
    "\n",
    "        self.model.train()\n",
    "        self.summary_writer.flush()\n",
    "\n",
    "    def get_running_results(self, ds):\n",
    "        mpjpe = np.array(self.evaluation_results_dict[ds]['MPJPE'])\n",
    "        pampjpe = np.array(self.evaluation_results_dict[ds]['PAMPJPE'])\n",
    "        mpjpe_mean, mpjpe_var, pampjpe_mean, pampjpe_var = np.mean(mpjpe), np.var(mpjpe), np.mean(pampjpe), np.var(pampjpe)\n",
    "        return mpjpe_mean, mpjpe_var, pampjpe_mean, pampjpe_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69a0ed-2a77-4200-aa8a-678ba7a239d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args, train_dataloader, val_dataloader, METRO_model, smpl, mesh_sampler, renderer):\n",
    "\n",
    "    max_iter = len(train_dataloader)\n",
    "    iters_per_epoch = max_iter // args.num_train_epochs\n",
    "    if iters_per_epoch<1000:\n",
    "        args.logging_steps = 500\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=list(METRO_model.parameters()),\n",
    "                                           lr=args.lr,\n",
    "                                           betas=(0.9, 0.999),\n",
    "                                           weight_decay=0)\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion_2d_keypoints = torch.nn.MSELoss(reduction='none').cuda(args.device)\n",
    "    criterion_keypoints = torch.nn.MSELoss(reduction='none').cuda(args.device)\n",
    "    criterion_vertices = torch.nn.L1Loss().cuda(args.device)\n",
    "\n",
    "\n",
    "    start_training_time = time.time()\n",
    "    end = time.time()\n",
    "    METRO_model.train()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    log_losses = AverageMeter()\n",
    "    log_loss_2djoints = AverageMeter()\n",
    "    log_loss_3djoints = AverageMeter()\n",
    "    log_loss_vertices = AverageMeter()\n",
    "    log_eval_metrics = EvalMetricsLogger()\n",
    "\n",
    "    for iteration, (img_keys, images, annotations) in enumerate(train_dataloader):\n",
    "\n",
    "        METRO_model.train()\n",
    "        iteration += 1\n",
    "        epoch = iteration // iters_per_epoch\n",
    "        batch_size = images.size(0)\n",
    "        adjust_learning_rate(optimizer, epoch, args)\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        images = images.cuda(args.device)\n",
    "                \n",
    "        \n",
    "        # forward-pass\n",
    "        pred_camera, pred_3d_joints, pred_vertices_sub2, pred_vertices_sub, pred_vertices = METRO_model(images, smpl, mesh_sampler, meta_masks=meta_masks, is_train=True)\n",
    "\n",
    "        \n",
    "        # compute 3d joint loss  (where the joints are directly output from transformer)\n",
    "        loss_3d_joints = keypoint_3d_loss(criterion_keypoints, pred_3d_joints, gt_3d_joints, has_3d_joints, args.device)\n",
    "        # compute 3d vertex loss\n",
    "        loss_vertices = ( args.vloss_w_sub2 * vertices_loss(criterion_vertices, pred_vertices_sub2, gt_vertices_sub2, has_smpl, args.device) + \\\n",
    "                            args.vloss_w_sub * vertices_loss(criterion_vertices, pred_vertices_sub, gt_vertices_sub, has_smpl, args.device) + \\\n",
    "                            args.vloss_w_full * vertices_loss(criterion_vertices, pred_vertices, gt_vertices, has_smpl, args.device) )\n",
    "        # compute 3d joint loss (where the joints are regressed from full mesh)\n",
    "        loss_reg_3d_joints = keypoint_3d_loss(criterion_keypoints, pred_3d_joints_from_smpl, gt_3d_joints, has_3d_joints, args.device)\n",
    "        # compute 2d joint loss\n",
    "        loss_2d_joints = keypoint_2d_loss(criterion_2d_keypoints, pred_2d_joints, gt_2d_joints, has_2d_joints)  + \\\n",
    "                         keypoint_2d_loss(criterion_2d_keypoints, pred_2d_joints_from_smpl, gt_2d_joints, has_2d_joints)\n",
    "        \n",
    "        loss_3d_joints = loss_3d_joints + loss_reg_3d_joints\n",
    "    \n",
    "        # we empirically use hyperparameters to balance difference losses\n",
    "        loss = args.joints_loss_weight*loss_3d_joints + \\\n",
    "                args.vertices_loss_weight*loss_vertices  + args.vertices_loss_weight*loss_2d_joints\n",
    "\n",
    "        # update logs\n",
    "        log_loss_2djoints.update(loss_2d_joints.item(), batch_size)\n",
    "        log_loss_3djoints.update(loss_3d_joints.item(), batch_size)\n",
    "        log_loss_vertices.update(loss_vertices.item(), batch_size)\n",
    "        log_losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # back prop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if iteration % args.logging_steps == 0 or iteration == max_iter:\n",
    "            eta_seconds = batch_time.avg * (max_iter - iteration)\n",
    "            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "            logger.info(\n",
    "                ' '.join(\n",
    "                ['eta: {eta}', 'epoch: {ep}', 'iter: {iter}', 'max mem : {memory:.0f}',]\n",
    "                ).format(eta=eta_string, ep=epoch, iter=iteration, \n",
    "                    memory=torch.cuda.max_memory_allocated() / 1024.0 / 1024.0) \n",
    "                + '  loss: {:.4f}, 2d joint loss: {:.4f}, 3d joint loss: {:.4f}, vertex loss: {:.4f}, compute: {:.4f}, data: {:.4f}, lr: {:.6f}'.format(\n",
    "                    log_losses.avg, log_loss_2djoints.avg, log_loss_3djoints.avg, log_loss_vertices.avg, batch_time.avg, data_time.avg, \n",
    "                    optimizer.param_groups[0]['lr'])\n",
    "            )\n",
    "\n",
    "            visual_imgs = visualize_mesh(   renderer,\n",
    "                                            annotations['ori_img'].detach(),\n",
    "                                            annotations['joints_2d'].detach(),\n",
    "                                            pred_vertices.detach(), \n",
    "                                            pred_camera.detach(),\n",
    "                                            pred_2d_joints_from_smpl.detach())\n",
    "            visual_imgs = visual_imgs.transpose(0,1)\n",
    "            visual_imgs = visual_imgs.transpose(1,2)\n",
    "            visual_imgs = np.asarray(visual_imgs)\n",
    "\n",
    "            if is_main_process()==True:\n",
    "                stamp = str(epoch) + '_' + str(iteration)\n",
    "                temp_fname = args.output_dir + 'visual_' + stamp + '.jpg'\n",
    "                cv2.imwrite(temp_fname, np.asarray(visual_imgs[:,:,::-1]*255))\n",
    "\n",
    "        if iteration % iters_per_epoch == 0:\n",
    "            val_mPVE, val_mPJPE, val_PAmPJPE, val_count = run_validate(args, val_dataloader, \n",
    "                                                METRO_model, \n",
    "                                                criterion_keypoints, \n",
    "                                                criterion_vertices, \n",
    "                                                epoch, \n",
    "                                                smpl,\n",
    "                                                mesh_sampler)\n",
    "\n",
    "            logger.info(\n",
    "                ' '.join(['Validation', 'epoch: {ep}',]).format(ep=epoch) \n",
    "                + '  mPVE: {:6.2f}, mPJPE: {:6.2f}, PAmPJPE: {:6.2f}, Data Count: {:6.2f}'.format(1000*val_mPVE, 1000*val_mPJPE, 1000*val_PAmPJPE, val_count)\n",
    "            )\n",
    "\n",
    "            if val_PAmPJPE<log_eval_metrics.PAmPJPE:\n",
    "                checkpoint_dir = save_checkpoint(METRO_model, args, epoch, iteration)\n",
    "                log_eval_metrics.update(val_mPVE, val_mPJPE, val_PAmPJPE, epoch)\n",
    "                \n",
    "        \n",
    "    total_training_time = time.time() - start_training_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=total_training_time))\n",
    "    logger.info('Total training time: {} ({:.4f} s / iter)'.format(\n",
    "        total_time_str, total_training_time / max_iter)\n",
    "    )\n",
    "    checkpoint_dir = save_checkpoint(METRO_model, args, epoch, iteration)\n",
    "\n",
    "    logger.info(\n",
    "        ' Best Results:'\n",
    "        + '  mPVE: {:6.2f}, mPJPE: {:6.2f}, PAmPJPE: {:6.2f}, at epoch {:6.2f}'.format(1000*log_eval_metrics.mPVE, 1000*log_eval_metrics.mPJPE, 1000*log_eval_metrics.PAmPJPE, log_eval_metrics.epoch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe6a9a-a43f-4aaf-af06-83a47ab40d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_encoder = []\n",
    "\n",
    "config_class, model_class = BertConfig, METRO\n",
    "config = config_class.from_pretrained(args.config_name if args.config_name \\\n",
    "        else args.model_name_or_path)\n",
    "\n",
    "config.output_attentions = False\n",
    "config.hidden_dropout_prob = args.drop_out\n",
    "config.img_feature_dim = input_feat_dim\n",
    "config.output_feature_dim = output_feat_dim\n",
    "args.hidden_size = hidden_feat_dim\n",
    "\n",
    "if args.legacy_setting==True:\n",
    "    # During our paper submission, we were using the original intermediate size, which is 3072 fixed\n",
    "    # We keep our legacy setting here \n",
    "    args.intermediate_size = -1\n",
    "else:\n",
    "    # We have recently tried to use an updated intermediate size, which is 4*hidden-size.\n",
    "    # But we didn't find significant performance changes on Human3.6M (~36.7 PA-MPJPE)\n",
    "    args.intermediate_size = int(args.hidden_size*4)\n",
    "\n",
    "# update model structure if specified in arguments\n",
    "update_params = ['num_hidden_layers', 'hidden_size', 'num_attention_heads', 'intermediate_size']\n",
    "\n",
    "for idx, param in enumerate(update_params):\n",
    "    arg_param = getattr(args, param)\n",
    "    config_param = getattr(config, param)\n",
    "    if arg_param > 0 and arg_param != config_param:\n",
    "        print(\"Update config parameter {}: {} -> {}\".format(param, config_param, arg_param))\n",
    "        setattr(config, param, arg_param)\n",
    "\n",
    "# init a transformer encoder and append it to a list\n",
    "assert config.hidden_size % config.num_attention_heads == 0\n",
    "model = model_class(config=config) \n",
    "print(\"Init model from scratch.\")\n",
    "trans_encoder.append(model)\n",
    "\n",
    "trans_encoder = torch.nn.Sequential(*trans_encoder)\n",
    "total_params = sum(p.numel() for p in trans_encoder.parameters())\n",
    "print('Transformers total parameters: {}'.format(total_params))\n",
    "backbone_total_params = sum(p.numel() for p in backbone.parameters())\n",
    "print('Backbone total parameters: {}'.format(backbone_total_params))\n",
    "\n",
    "# build end-to-end network (CNN backbone + transformer encoder + transformer decoder)\n",
    "_metro_network = Network(args, config, backbone, trans_encoder, mesh_sampler)\n",
    "\n",
    "_metro_network.to(args.device)\n",
    "print(\"Training parameters %s\", args)\n",
    "\n",
    "train_dataloader = make_data_loader(args, args.train_yaml, args.distributed,\n",
    "                                    is_train=True, scale_factor=args.img_scale_factor)\n",
    "val_dataloader = make_data_loader(args, args.val_yaml, args.distributed,\n",
    "                                    is_train=False, scale_factor=args.img_scale_factor)\n",
    "run(args, train_dataloader, val_dataloader, _metro_network, smpl, mesh_sampler, renderer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0b475-fbb3-47ec-9669-c2f009a2e9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc05a1d4-c763-4779-9f20-31981626cef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b87eb-f08f-4eaa-9e44-3a844103f0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679b8dd-e3b6-49e4-b6fe-e53e8d2e74a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
